{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-11T21:50:00.950243Z",
     "iopub.status.busy": "2025-06-11T21:50:00.949882Z",
     "iopub.status.idle": "2025-06-11T21:50:01.400974Z",
     "shell.execute_reply": "2025-06-11T21:50:01.400251Z",
     "shell.execute_reply.started": "2025-06-11T21:50:00.950216Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /kaggle/input/amini-soil-prediction-challenge-dataset\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"muhammadqasimshabbir/amini-soil-prediction-challenge-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T21:44:18.125845Z",
     "iopub.status.busy": "2025-06-22T21:44:18.125475Z",
     "iopub.status.idle": "2025-06-22T23:00:48.961208Z",
     "shell.execute_reply": "2025-06-22T23:00:48.960321Z",
     "shell.execute_reply.started": "2025-06-22T21:44:18.125817Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading core data...\n",
      "\n",
      "--- Adding Missing Indicator Flags & Capping Outliers ---\n",
      "\n",
      "--- Starting Satellite Data Integration ---\n",
      "Processing satellite file: LANDSAT8_data_updated.csv\n",
      "Processing satellite file: MODIS_MOD16A2_data.csv\n",
      "Processing satellite file: MODIS_MCD43A4_data.csv\n",
      "Processing satellite file: MODIS_MOD09GA_data.csv\n",
      "Processing satellite file: Sentinel2_data.csv\n",
      "Processing satellite file: Sentinel1_data.csv\n",
      "Processing satellite file: MODIS_MOD13Q1_data.csv\n",
      "Processing satellite file: MODIS_MOD11A1_data.csv\n",
      "\n",
      "Finished satellite data integration.\n",
      "\n",
      "--- Advanced Static and EO Interaction Feature Engineering ---\n",
      "\n",
      "--- Final Preprocessing ---\n",
      "  Performing final median imputation...\n",
      "  Dropping 64 features (all NaN or no variance in train): ['LANDSAT8_QA_RADSAT_knn_mean_k3_win30d_recent', 'LANDSAT8_QA_RADSAT_knn_mean_k3_win90d_mid', 'LANDSAT8_QA_RADSAT_knn_mean_k3_win180d_early', 'LANDSAT8_QA_RADSAT_knn_mean_k3_win_full_year', 'Sentinel2_B1_knn_mean_k3_win30d_recent', 'Sentinel2_B11_knn_mean_k3_win30d_recent', 'Sentinel2_B12_knn_mean_k3_win30d_recent', 'Sentinel2_B2_knn_mean_k3_win30d_recent', 'Sentinel2_B3_knn_mean_k3_win30d_recent', 'Sentinel2_B4_knn_mean_k3_win30d_recent', 'Sentinel2_B5_knn_mean_k3_win30d_recent', 'Sentinel2_B6_knn_mean_k3_win30d_recent', 'Sentinel2_B7_knn_mean_k3_win30d_recent', 'Sentinel2_B8_knn_mean_k3_win30d_recent', 'Sentinel2_B8A_knn_mean_k3_win30d_recent', 'Sentinel2_B9_knn_mean_k3_win30d_recent', 'Sentinel2_CLOUDY_PIXEL_PERCENTAGE_knn_mean_k3_win30d_recent', 'Sentinel2_MEAN_SOLAR_ZENITH_ANGLE_knn_mean_k3_win30d_recent', 'Sentinel2_NODATA_PIXEL_PERCENTAGE_knn_mean_k3_win30d_recent', 'Sentinel2_SENSING_ORBIT_NUMBER_knn_mean_k3_win30d_recent', 'Sentinel2_lat_knn_mean_k3_win30d_recent', 'Sentinel2_lon_knn_mean_k3_win30d_recent', 'Sentinel2_NDVI_calc_knn_mean_k3_win30d_recent', 'Sentinel2_NODATA_PIXEL_PERCENTAGE_knn_mean_k3_win180d_early', 'Sentinel2_SENSING_ORBIT_NUMBER_knn_mean_k3_win180d_early', 'Sentinel2_NDVI_calc_knn_mean_k3_win30d_recent_is_missing', 'Sentinel2_lat_knn_mean_k3_win30d_recent_is_missing', 'Sentinel2_B5_knn_mean_k3_win30d_recent_is_missing', 'Sentinel2_B7_knn_mean_k3_win30d_recent_is_missing', 'Sentinel2_B9_knn_mean_k3_win30d_recent_is_missing', 'Sentinel2_B2_knn_mean_k3_win30d_recent_is_missing', 'Sentinel2_SENSING_ORBIT_NUMBER_knn_mean_k3_win30d_recent_is_missing', 'Sentinel2_B8A_knn_mean_k3_win30d_recent_is_missing', 'Sentinel2_B4_knn_mean_k3_win30d_recent_is_missing', 'Sentinel2_CLOUDY_PIXEL_PERCENTAGE_knn_mean_k3_win30d_recent_is_missing', 'Sentinel2_NODATA_PIXEL_PERCENTAGE_knn_mean_k3_win30d_recent_is_missing', 'Sentinel2_B1_knn_mean_k3_win30d_recent_is_missing', 'Sentinel2_MEAN_SOLAR_ZENITH_ANGLE_knn_mean_k3_win30d_recent_is_missing', 'Sentinel2_B11_knn_mean_k3_win30d_recent_is_missing', 'Sentinel2_B3_knn_mean_k3_win30d_recent_is_missing', 'Sentinel2_lon_knn_mean_k3_win30d_recent_is_missing', 'Sentinel2_B8_knn_mean_k3_win30d_recent_is_missing', 'Sentinel2_B6_knn_mean_k3_win30d_recent_is_missing', 'Sentinel2_B12_knn_mean_k3_win30d_recent_is_missing', 'Sentinel1_VV_knn_mean_k3_win30d_recent_is_missing', 'Sentinel1_lon_knn_mean_k3_win180d_early_is_missing', 'Sentinel1_lon_knn_mean_k3_win90d_mid_is_missing', 'Sentinel1_relativeOrbitNumber_start_knn_mean_k3_win90d_mid_is_missing', 'Sentinel1_VH_knn_mean_k3_win_full_year_is_missing', 'Sentinel1_VV_knn_mean_k3_win180d_early_is_missing', 'Sentinel1_lat_knn_mean_k3_win30d_recent_is_missing', 'Sentinel1_VV_knn_mean_k3_win_full_year_is_missing', 'Sentinel1_relativeOrbitNumber_start_knn_mean_k3_win30d_recent_is_missing', 'Sentinel1_lon_knn_mean_k3_win30d_recent_is_missing', 'Sentinel1_VH_knn_mean_k3_win180d_early_is_missing', 'Sentinel1_VV_knn_mean_k3_win90d_mid_is_missing', 'Sentinel1_lat_knn_mean_k3_win90d_mid_is_missing', 'Sentinel1_relativeOrbitNumber_start_knn_mean_k3_win180d_early_is_missing', 'Sentinel1_VH_knn_mean_k3_win90d_mid_is_missing', 'Sentinel1_lat_knn_mean_k3_win_full_year_is_missing', 'Sentinel1_lon_knn_mean_k3_win_full_year_is_missing', 'Sentinel1_relativeOrbitNumber_start_knn_mean_k3_win_full_year_is_missing', 'Sentinel1_lat_knn_mean_k3_win180d_early_is_missing', 'Sentinel1_VH_knn_mean_k3_win30d_recent_is_missing']\n",
      "Number of features for modeling: 557\n",
      "  Scaling features for GLM using StandardScaler...\n",
      "\n",
      "--- Training LightGBM Models ---\n",
      "  Training LGBM for Nutrient: N\n",
      "    Applying confident pseudo-labeling for LGBM on N...\n",
      "    Starting pseudo-labeling. Initial train size: 7744\n",
      "      Pseudo-labeling iteration 1/3\n",
      "        Found 484 confident pseudo-labels (percentile: 80th).\n",
      "        New train size: 8228\n",
      "      Pseudo-labeling iteration 2/3\n",
      "        Found 484 confident pseudo-labels (percentile: 80th).\n",
      "        New train size: 8712\n",
      "      Pseudo-labeling iteration 3/3\n",
      "        Found 484 confident pseudo-labels (percentile: 80th).\n",
      "        New train size: 9196\n",
      "    Fitting final model on augmented data of size 9196\n",
      "  Training LGBM for Nutrient: P\n",
      "    Applying confident pseudo-labeling for LGBM on P...\n",
      "    Starting pseudo-labeling. Initial train size: 7744\n",
      "      Pseudo-labeling iteration 1/3\n",
      "        Found 484 confident pseudo-labels (percentile: 80th).\n",
      "        New train size: 8228\n",
      "      Pseudo-labeling iteration 2/3\n",
      "        Found 484 confident pseudo-labels (percentile: 80th).\n",
      "        New train size: 8712\n",
      "      Pseudo-labeling iteration 3/3\n",
      "        Found 484 confident pseudo-labels (percentile: 80th).\n",
      "        New train size: 9196\n",
      "    Fitting final model on augmented data of size 9196\n",
      "  Training LGBM for Nutrient: K\n",
      "    Applying confident pseudo-labeling for LGBM on K...\n",
      "    Starting pseudo-labeling. Initial train size: 7744\n",
      "      Pseudo-labeling iteration 1/3\n",
      "        Found 484 confident pseudo-labels (percentile: 80th).\n",
      "        New train size: 8228\n",
      "      Pseudo-labeling iteration 2/3\n",
      "        Found 484 confident pseudo-labels (percentile: 80th).\n",
      "        New train size: 8712\n",
      "      Pseudo-labeling iteration 3/3\n",
      "        Found 484 confident pseudo-labels (percentile: 80th).\n",
      "        New train size: 9196\n",
      "    Fitting final model on augmented data of size 9196\n",
      "  Training LGBM for Nutrient: Ca\n",
      "    Applying confident pseudo-labeling for LGBM on Ca...\n",
      "    Starting pseudo-labeling. Initial train size: 7744\n",
      "      Pseudo-labeling iteration 1/3\n",
      "        Found 484 confident pseudo-labels (percentile: 80th).\n",
      "        New train size: 8228\n",
      "      Pseudo-labeling iteration 2/3\n",
      "        Found 484 confident pseudo-labels (percentile: 80th).\n",
      "        New train size: 8712\n",
      "      Pseudo-labeling iteration 3/3\n",
      "        Found 484 confident pseudo-labels (percentile: 80th).\n",
      "        New train size: 9196\n",
      "    Fitting final model on augmented data of size 9196\n",
      "  Training LGBM for Nutrient: Mg\n",
      "    Applying confident pseudo-labeling for LGBM on Mg...\n",
      "    Starting pseudo-labeling. Initial train size: 7744\n",
      "      Pseudo-labeling iteration 1/3\n",
      "        Found 484 confident pseudo-labels (percentile: 80th).\n",
      "        New train size: 8228\n",
      "      Pseudo-labeling iteration 2/3\n",
      "        Found 484 confident pseudo-labels (percentile: 80th).\n",
      "        New train size: 8712\n",
      "      Pseudo-labeling iteration 3/3\n",
      "        Found 484 confident pseudo-labels (percentile: 80th).\n",
      "        New train size: 9196\n",
      "    Fitting final model on augmented data of size 9196\n",
      "  Training LGBM for Nutrient: S\n",
      "    Applying confident pseudo-labeling for LGBM on S...\n",
      "    Starting pseudo-labeling. Initial train size: 7744\n",
      "      Pseudo-labeling iteration 1/3\n",
      "        Found 484 confident pseudo-labels (percentile: 80th).\n",
      "        New train size: 8228\n",
      "      Pseudo-labeling iteration 2/3\n",
      "        Found 484 confident pseudo-labels (percentile: 80th).\n",
      "        New train size: 8712\n",
      "      Pseudo-labeling iteration 3/3\n",
      "        Found 484 confident pseudo-labels (percentile: 80th).\n",
      "        New train size: 9196\n",
      "    Fitting final model on augmented data of size 9196\n",
      "  Training LGBM for Nutrient: Zn\n",
      "    Applying confident pseudo-labeling for LGBM on Zn...\n",
      "    Starting pseudo-labeling. Initial train size: 7744\n",
      "      Pseudo-labeling iteration 1/3\n",
      "        Found 484 confident pseudo-labels (percentile: 80th).\n",
      "        New train size: 8228\n",
      "      Pseudo-labeling iteration 2/3\n",
      "        Found 484 confident pseudo-labels (percentile: 80th).\n",
      "        New train size: 8712\n",
      "      Pseudo-labeling iteration 3/3\n",
      "        Found 484 confident pseudo-labels (percentile: 80th).\n",
      "        New train size: 9196\n",
      "    Fitting final model on augmented data of size 9196\n",
      "  Training LGBM for Nutrient: Mn\n",
      "    Applying confident pseudo-labeling for LGBM on Mn...\n",
      "    Starting pseudo-labeling. Initial train size: 7744\n",
      "      Pseudo-labeling iteration 1/3\n",
      "        Found 484 confident pseudo-labels (percentile: 80th).\n",
      "        New train size: 8228\n",
      "      Pseudo-labeling iteration 2/3\n",
      "        Found 484 confident pseudo-labels (percentile: 80th).\n",
      "        New train size: 8712\n",
      "      Pseudo-labeling iteration 3/3\n",
      "        Found 484 confident pseudo-labels (percentile: 80th).\n",
      "        New train size: 9196\n",
      "    Fitting final model on augmented data of size 9196\n",
      "  Training LGBM for Nutrient: Fe\n",
      "    Applying confident pseudo-labeling for LGBM on Fe...\n",
      "    Starting pseudo-labeling. Initial train size: 7744\n",
      "      Pseudo-labeling iteration 1/3\n",
      "        Found 484 confident pseudo-labels (percentile: 80th).\n",
      "        New train size: 8228\n",
      "      Pseudo-labeling iteration 2/3\n",
      "        Found 484 confident pseudo-labels (percentile: 80th).\n",
      "        New train size: 8712\n",
      "      Pseudo-labeling iteration 3/3\n",
      "        Found 484 confident pseudo-labels (percentile: 80th).\n",
      "        New train size: 9196\n",
      "    Fitting final model on augmented data of size 9196\n",
      "  Training LGBM for Nutrient: Cu\n",
      "    Applying confident pseudo-labeling for LGBM on Cu...\n",
      "    Starting pseudo-labeling. Initial train size: 7744\n",
      "      Pseudo-labeling iteration 1/3\n",
      "        Found 484 confident pseudo-labels (percentile: 80th).\n",
      "        New train size: 8228\n",
      "      Pseudo-labeling iteration 2/3\n",
      "        Found 484 confident pseudo-labels (percentile: 80th).\n",
      "        New train size: 8712\n",
      "      Pseudo-labeling iteration 3/3\n",
      "        Found 484 confident pseudo-labels (percentile: 80th).\n",
      "        New train size: 9196\n",
      "    Fitting final model on augmented data of size 9196\n",
      "  Training LGBM for Nutrient: B\n",
      "    Applying confident pseudo-labeling for LGBM on B...\n",
      "    Starting pseudo-labeling. Initial train size: 7744\n",
      "      Pseudo-labeling iteration 1/3\n",
      "        Found 484 confident pseudo-labels (percentile: 80th).\n",
      "        New train size: 8228\n",
      "      Pseudo-labeling iteration 2/3\n",
      "        Found 484 confident pseudo-labels (percentile: 80th).\n",
      "        New train size: 8712\n",
      "      Pseudo-labeling iteration 3/3\n",
      "        Found 484 confident pseudo-labels (percentile: 80th).\n",
      "        New train size: 9196\n",
      "    Fitting final model on augmented data of size 9196\n",
      "\n",
      "--- Training GLM (TweedieRegressor) Models ---\n",
      "  Training GLM for Nutrient: N\n",
      "  Training GLM for Nutrient: P\n",
      "  Training GLM for Nutrient: K\n",
      "  Training GLM for Nutrient: Ca\n",
      "  Training GLM for Nutrient: Mg\n",
      "  Training GLM for Nutrient: S\n",
      "  Training GLM for Nutrient: Zn\n",
      "  Training GLM for Nutrient: Mn\n",
      "  Training GLM for Nutrient: Fe\n",
      "  Training GLM for Nutrient: Cu\n",
      "  Training GLM for Nutrient: B\n",
      "\n",
      "--- Ensembling Predictions and Generating Submission ---\n",
      "Debug section 8 - PIDs dtypes before merge 1:\n",
      "  test_pred_ppm_long['PID']: object\n",
      "  original_test_bd_df['PID']: object\n",
      "Debug section 8 - PIDs/Nutrient dtypes before merge 2:\n",
      "  final_submission_prep_df['PID']: object\n",
      "  final_submission_prep_df['Nutrient']: object\n",
      "  gap_test_df_orig['PID']: object\n",
      "  gap_test_df_orig['Nutrient']: object\n",
      "\n",
      "Submission file 'submission_full_v_simpler_pl_robust_pid.csv' created successfully.\n",
      "Number of rows in submission: 26598\n",
      "            ID          Gap\n",
      "0  ID_NGS9Bx_N -3373.602217\n",
      "1  ID_YdVKXw_N -3571.080356\n",
      "2  ID_MZAlfE_N -3459.200202\n",
      "3  ID_GwCCMN_N -3561.372596\n",
      "4  ID_K8sowf_N -4238.862292\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, PolynomialFeatures, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import TweedieRegressor\n",
    "\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "import warnings\n",
    "from datetime import timedelta, datetime\n",
    "import itertools\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_PATH = '/kaggle/input/amini-soil-prediction-challenge-dataset/'\n",
    "TARGET_NUTRIENTS = ['N', 'P', 'K', 'Ca', 'Mg', 'S', 'Zn', 'Mn', 'Fe', 'Cu', 'B']\n",
    "SOIL_DEPTH_CM = 20\n",
    "SEED = 42\n",
    "K_FOR_KNN_FEATURES = 3\n",
    "\n",
    "REFERENCE_YEAR = 2019\n",
    "REFERENCE_END_DATE_OF_MAIN_WINDOW_STR = f\"{REFERENCE_YEAR}-08-01\"\n",
    "SATELLITE_TIME_WINDOWS = [\n",
    "    (\"win30d_recent\", 30, 0), (\"win90d_mid\", 90, 30),\n",
    "    (\"win180d_early\", 180, 90), (\"win_full_year\", 365, 0)\n",
    "]\n",
    "\n",
    "# Outlier Capping Config\n",
    "FEATURES_FOR_OUTLIER_CAPPING = [\n",
    "    'ls', 'soc20', 'slope', 'mb1', 'mb2', 'mb3', 'mb7', 'parv', 'hp20',\n",
    "]\n",
    "LOWER_PERCENTILE_CAP = 0.01\n",
    "UPPER_PERCENTILE_CAP = 0.99\n",
    "\n",
    "# Ensemble Weights\n",
    "WEIGHT_LGBM = 0.7\n",
    "WEIGHT_GLM = 0.3\n",
    "\n",
    "# --- Pseudo-Labeling Configuration (Using Simpler Version Parameters) ---\n",
    "ENABLE_PSEUDO_LABELING_LGBM = True\n",
    "PSEUDO_LABELING_ITERATIONS = 3\n",
    "PSEUDO_LABELING_CONFIDENCE_PERCENTILE = 80 # As in your simpler working example\n",
    "PSEUDO_LABELING_N_BOOTSTRAP_CONF = 3 # As in your simpler working example\n",
    "PSEUDO_LABELING_MIN_SAMPLES = 10 # As in your simpler working example\n",
    "\n",
    "# --- Pseudo-Labeling Helper Functions (Simpler Version from your \"Working Code\") ---\n",
    "def calculate_confidence_lgbm_bootstrap(base_model_params,\n",
    "                                        X_train_df_conf, y_train_series_conf,\n",
    "                                        X_test_df_conf,\n",
    "                                        selected_features,\n",
    "                                        n_bootstrap_models=3, seed_offset=100, base_seed=42):\n",
    "    predictions_bootstrap = []\n",
    "    \n",
    "    X_train_subset_np = X_train_df_conf[selected_features].values\n",
    "    y_train_subset_np = y_train_series_conf.values\n",
    "    X_test_subset_np = X_test_df_conf[selected_features].values\n",
    "\n",
    "    if X_train_subset_np.shape[0] == 0:\n",
    "        print(\"      Warning: Empty training data for bootstrap. Returning zero confidence.\")\n",
    "        return np.zeros(X_test_subset_np.shape[0]) # Return only confidence for this simpler PL\n",
    "\n",
    "    for i in range(n_bootstrap_models):\n",
    "        n_samples = X_train_subset_np.shape[0]\n",
    "        # Standard unweighted bootstrap\n",
    "        bootstrap_indices = np.random.choice(np.arange(n_samples), size=n_samples, replace=True)\n",
    "        \n",
    "        X_boot = X_train_subset_np[bootstrap_indices]\n",
    "        y_boot = y_train_subset_np[bootstrap_indices]\n",
    "        \n",
    "        model_boot_params = base_model_params.copy()\n",
    "        model_boot_params['seed'] = base_seed + seed_offset + i\n",
    "        model_boot_params['verbose'] = -1\n",
    "        \n",
    "        model_boot = lgb.LGBMRegressor(**model_boot_params)\n",
    "        try:\n",
    "            model_boot.fit(X_boot, y_boot) # No sample_weight here for simpler PL\n",
    "            preds_boot = model_boot.predict(X_test_subset_np)\n",
    "        except Exception as e_boot:\n",
    "            print(f\"      Warning: Bootstrap model {i+1} fit/predict failed: {e_boot}. Using zeros for this bootstrap.\")\n",
    "            preds_boot = np.zeros(X_test_subset_np.shape[0])\n",
    "        predictions_bootstrap.append(preds_boot)\n",
    "\n",
    "    pred_array = np.array(predictions_bootstrap)\n",
    "    \n",
    "    if pred_array.shape[0] < 2 and n_bootstrap_models > 0:\n",
    "        print(\"      Warning: Not enough successful bootstrap models for std_dev. Returning low confidence.\")\n",
    "        return np.full(X_test_subset_np.shape[0], 1e-6) # Low confidence\n",
    "    elif n_bootstrap_models == 0:\n",
    "         return np.ones(X_test_subset_np.shape[0]) # Neutral confidence\n",
    "\n",
    "    std_dev = np.std(pred_array, axis=0)\n",
    "    confidence = 1.0 / (std_dev + 1e-6)\n",
    "    return confidence # Return only confidence\n",
    "\n",
    "def confident_pseudo_labeling_lgbm( # This is the simpler version\n",
    "    initial_model_params,\n",
    "    X_train_orig_df, y_train_orig_series,\n",
    "    X_test_orig_df, # This is the features of the test set where we want to generate pseudo-labels\n",
    "    selected_features_list,\n",
    "    iterations=5,\n",
    "    confidence_percentile=80,\n",
    "    n_bootstrap_for_confidence=3,\n",
    "    base_seed=42,\n",
    "    min_confident_samples_to_add=1\n",
    "):\n",
    "    current_X_train_df = X_train_orig_df.copy()\n",
    "    current_y_train_series = y_train_orig_series.copy()\n",
    "    # No sample weights in this simpler version\n",
    "\n",
    "    print(f\"    Starting pseudo-labeling. Initial train size: {len(current_X_train_df)}\")\n",
    "\n",
    "    for i in range(iterations):\n",
    "        print(f\"      Pseudo-labeling iteration {i+1}/{iterations}\")\n",
    "        \n",
    "        iter_model_params = initial_model_params.copy()\n",
    "        current_model_for_preds = lgb.LGBMRegressor(**iter_model_params)\n",
    "        # Fit on current training data (no weights)\n",
    "        current_model_for_preds.fit(\n",
    "            current_X_train_df[selected_features_list], \n",
    "            current_y_train_series\n",
    "        )\n",
    "        \n",
    "        # Predict on original test data (X_test_orig_df) to get potential pseudo-labels\n",
    "        test_preds = current_model_for_preds.predict(X_test_orig_df[selected_features_list])\n",
    "        \n",
    "        # Calculate prediction confidence using bootstrap models trained on current_X_train_df\n",
    "        confidence = calculate_confidence_lgbm_bootstrap(\n",
    "            initial_model_params,\n",
    "            current_X_train_df, # Current augmented training data\n",
    "            current_y_train_series, # Current augmented training labels\n",
    "            X_test_orig_df,       # Original test data for confidence estimation\n",
    "            selected_features_list,\n",
    "            n_bootstrap_models=n_bootstrap_for_confidence,\n",
    "            seed_offset=1000 * (i + 1),\n",
    "            base_seed=base_seed\n",
    "        )\n",
    "        \n",
    "        finite_confidence = confidence[np.isfinite(confidence)]\n",
    "        if len(finite_confidence) == 0:\n",
    "            print(\"        All confidence values are non-finite. Stopping pseudo-labeling.\")\n",
    "            break\n",
    "        \n",
    "        confidence_threshold = np.percentile(finite_confidence, confidence_percentile)\n",
    "        # Create mask based on X_test_orig_df's length\n",
    "        confident_mask = (confidence >= confidence_threshold) & np.isfinite(confidence)\n",
    "        \n",
    "        num_confident_samples = np.sum(confident_mask)\n",
    "\n",
    "        if num_confident_samples < min_confident_samples_to_add:\n",
    "            print(f\"        Found only {num_confident_samples} confident pseudo-labels (min required: {min_confident_samples_to_add}). Stopping.\")\n",
    "            break\n",
    "        \n",
    "        print(f\"        Found {num_confident_samples} confident pseudo-labels (percentile: {confidence_percentile}th).\")\n",
    "\n",
    "        # Get pseudo-labeled data (features from X_test_orig_df, labels are test_preds)\n",
    "        X_pseudo_df = X_test_orig_df[selected_features_list][confident_mask].copy()\n",
    "        # Create a Series for pseudo labels, ensuring correct indexing from X_pseudo_df\n",
    "        y_pseudo_series = pd.Series(test_preds[confident_mask], index=X_pseudo_df.index, name=current_y_train_series.name)\n",
    "        \n",
    "        # Add confident pseudo-labels to the training set\n",
    "        # ignore_index=True is critical for this simpler version as it avoids index conflicts\n",
    "        current_X_train_df = pd.concat([current_X_train_df, X_pseudo_df], ignore_index=True)\n",
    "        current_y_train_series = pd.concat([current_y_train_series, y_pseudo_series], ignore_index=True)\n",
    "        \n",
    "        print(f\"        New train size: {len(current_X_train_df)}\")\n",
    "        gc.collect()\n",
    "\n",
    "    final_model_params = initial_model_params.copy()\n",
    "    final_model = lgb.LGBMRegressor(**final_model_params)\n",
    "    print(f\"    Fitting final model on augmented data of size {len(current_X_train_df)}\")\n",
    "    final_model.fit(\n",
    "        current_X_train_df[selected_features_list], \n",
    "        current_y_train_series # No sample_weight\n",
    "    )\n",
    "    \n",
    "    return final_model\n",
    "\n",
    "\n",
    "# --- Satellite Data Processing Function (QA Masking Implemented) ---\n",
    "def load_and_preprocess_satellite_data_multi_window_knn_features(\n",
    "    filepath, all_pids_from_base, reference_end_date_main_str,\n",
    "    time_windows_config, k_neighbors\n",
    "):\n",
    "    # print(f\"Processing satellite data (Temporal Aggregates, k={k_neighbors}): {filepath.split('/')[-1]}\") # Less verbose\n",
    "    try:\n",
    "        sat_df_full = pd.read_csv(filepath)\n",
    "    except FileNotFoundError:\n",
    "        # print(f\"File not found: {filepath}. Returning empty placeholder.\")\n",
    "        return pd.DataFrame(index=pd.Index(all_pids_from_base, name='PID'))\n",
    "\n",
    "    filename_prefix = filepath.split('/')[-1].split('.')[0].replace('_data_updated','').replace('_data','')\n",
    "    \n",
    "    date_col_sat = None\n",
    "    possible_date_cols = ['Date', 'system:time_start', 'DATE_ACQUIRED', 'sensing_time', 'date']\n",
    "    for col in possible_date_cols:\n",
    "        if col in sat_df_full.columns:\n",
    "            date_col_sat = col\n",
    "            break\n",
    "    if date_col_sat is None:\n",
    "        # print(f\"No date column found in {filepath}. Returning empty placeholder.\")\n",
    "        return pd.DataFrame(index=pd.Index(all_pids_from_base, name='PID'))\n",
    "\n",
    "    if sat_df_full[date_col_sat].dtype == 'object' or pd.api.types.is_string_dtype(sat_df_full[date_col_sat]):\n",
    "        try: sat_df_full[date_col_sat] = pd.to_datetime(sat_df_full[date_col_sat], errors='raise')\n",
    "        except: sat_df_full[date_col_sat] = pd.to_datetime(sat_df_full[date_col_sat].astype(str).str.split('T').str[0], errors='coerce')\n",
    "    elif pd.api.types.is_numeric_dtype(sat_df_full[date_col_sat]):\n",
    "        try: sat_df_full[date_col_sat] = pd.to_datetime(sat_df_full[date_col_sat], unit='ms', errors='coerce')\n",
    "        except: sat_df_full[date_col_sat] = pd.to_datetime(sat_df_full[date_col_sat], errors='coerce')\n",
    "    \n",
    "    if sat_df_full[date_col_sat].isnull().all():\n",
    "        # print(f\"All date values are null in {filepath}. Returning empty placeholder.\")\n",
    "        return pd.DataFrame(index=pd.Index(all_pids_from_base, name='PID'))\n",
    "\n",
    "    # Ensure PID is string for consistent merging from the start\n",
    "    sat_df_full['PID'] = sat_df_full['PID'].astype(str)\n",
    "    all_pids_from_base_str = [str(p) for p in all_pids_from_base]\n",
    "\n",
    "\n",
    "    sat_df_filtered_base = sat_df_full.dropna(subset=[date_col_sat, 'PID'])\n",
    "    # Filter by PIDs present in the main dataset (all_pids_from_base_str)\n",
    "    sat_df_filtered_base = sat_df_filtered_base[sat_df_filtered_base['PID'].isin(all_pids_from_base_str)].copy()\n",
    "\n",
    "\n",
    "    if sat_df_filtered_base.empty:\n",
    "        # print(f\"No data for relevant PIDs in {filepath}. Returning empty placeholder.\")\n",
    "        return pd.DataFrame(index=pd.Index(all_pids_from_base_str, name='PID')) # Use string PIDs for index\n",
    "\n",
    "    ndvi_base_name = \"NDVI_calc\"\n",
    "    # Define potential_base_feature_names before QA masking, based on original columns\n",
    "    potential_base_feature_names = [\n",
    "        c for c in sat_df_full.columns \n",
    "        if c not in ['PID', date_col_sat, 'QA_PIXEL', 'SCL'] # Exclude QA columns too\n",
    "        and pd.api.types.is_numeric_dtype(sat_df_full[c])\n",
    "    ]\n",
    "    if filename_prefix.startswith(\"LANDSAT8\") or filename_prefix.startswith(\"Sentinel2\"):\n",
    "        nir_band_check, red_band_check = ('SR_B5', 'SR_B4') if filename_prefix.startswith(\"LANDSAT8\") else ('B8', 'B4')\n",
    "        if nir_band_check in sat_df_full.columns and red_band_check in sat_df_full.columns:\n",
    "            if ndvi_base_name not in potential_base_feature_names: # Add if NDVI calculable\n",
    "                potential_base_feature_names.append(ndvi_base_name)\n",
    "\n",
    "\n",
    "    if filename_prefix.startswith(\"LANDSAT8\") and 'QA_PIXEL' in sat_df_filtered_base.columns:\n",
    "        qa_pixel_int = sat_df_filtered_base['QA_PIXEL'].astype(int)\n",
    "        is_clear_or_water = ((qa_pixel_int & (1 << 6)) != 0) | ((qa_pixel_int & (1 << 7)) != 0)\n",
    "        is_not_cloud_related_or_fill = (\n",
    "            ((qa_pixel_int & (1 << 3)) == 0) & ((qa_pixel_int & (1 << 4)) == 0) &\n",
    "            ((qa_pixel_int & (1 << 2)) == 0) & ((qa_pixel_int & (1 << 0)) == 0)   \n",
    "        )\n",
    "        good_pixel_mask_l8 = is_clear_or_water & is_not_cloud_related_or_fill\n",
    "        sat_df_filtered_base = sat_df_filtered_base[good_pixel_mask_l8]\n",
    "\n",
    "    if filename_prefix.startswith(\"Sentinel2\") and 'SCL' in sat_df_filtered_base.columns:\n",
    "        good_scl_values = [4, 5, 6, 7, 11] \n",
    "        scl_mask = sat_df_filtered_base['SCL'].isin(good_scl_values)\n",
    "        sat_df_filtered_base = sat_df_filtered_base[scl_mask]\n",
    "\n",
    "    # This placeholder creation needs to happen *after* potential_base_feature_names is defined\n",
    "    # And it should use all_pids_from_base_str for the index\n",
    "    def create_placeholder_df(pids_list, prefix, features_list, windows_cfg, k):\n",
    "        final_placeholder = pd.DataFrame({'PID': pids_list})\n",
    "        for win_name, _, _ in windows_cfg:\n",
    "            for feat_base in features_list:\n",
    "                final_placeholder[f\"{prefix}_{feat_base}_knn_mean_k{k}_{win_name}\"] = np.nan\n",
    "        return final_placeholder\n",
    "\n",
    "    if sat_df_filtered_base.empty:\n",
    "        # print(f\"No data remaining for {filename_prefix} after QA masking.\")\n",
    "        return create_placeholder_df(all_pids_from_base_str, filename_prefix, potential_base_feature_names, time_windows_config, k_neighbors)\n",
    "\n",
    "\n",
    "    if filename_prefix.startswith(\"LANDSAT8\") or filename_prefix.startswith(\"Sentinel2\"):\n",
    "        nir_band, red_band = ('SR_B5', 'SR_B4') if filename_prefix.startswith(\"LANDSAT8\") else ('B8', 'B4')\n",
    "        if nir_band in sat_df_filtered_base.columns and red_band in sat_df_filtered_base.columns:\n",
    "            if sat_df_filtered_base[nir_band].notna().any() and sat_df_filtered_base[red_band].notna().any():\n",
    "                numerator = sat_df_filtered_base[nir_band].astype(float) - sat_df_filtered_base[red_band].astype(float)\n",
    "                denominator = (sat_df_filtered_base[nir_band].astype(float) + sat_df_filtered_base[red_band].astype(float)).replace(0, np.nan)\n",
    "                sat_df_filtered_base[ndvi_base_name] = numerator / denominator\n",
    "                sat_df_filtered_base[ndvi_base_name].fillna(0, inplace=True)\n",
    "            else:\n",
    "                 sat_df_filtered_base[ndvi_base_name] = np.nan\n",
    "        elif ndvi_base_name in potential_base_feature_names: # If NDVI was expected but bands missing post-QA\n",
    "            sat_df_filtered_base[ndvi_base_name] = np.nan\n",
    "\n",
    "\n",
    "    feature_cols_sat_to_process = [\n",
    "        c for c in sat_df_filtered_base.columns \n",
    "        if pd.api.types.is_numeric_dtype(sat_df_filtered_base[c]) and \n",
    "           c not in ['PID', date_col_sat, 'QA_PIXEL', 'SCL']\n",
    "    ]\n",
    "    if ndvi_base_name in sat_df_filtered_base.columns and ndvi_base_name not in feature_cols_sat_to_process:\n",
    "        feature_cols_sat_to_process.append(ndvi_base_name)\n",
    "\n",
    "    if not feature_cols_sat_to_process:\n",
    "        # print(f\"No numeric features to process for {filename_prefix} after QA/NDVI.\")\n",
    "        return create_placeholder_df(all_pids_from_base_str, filename_prefix, potential_base_feature_names, time_windows_config, k_neighbors)\n",
    "\n",
    "    sat_df_filtered_base.sort_values(by=['PID', date_col_sat], inplace=True)\n",
    "    all_window_features_dfs = []\n",
    "    reference_end_date_main = pd.to_datetime(reference_end_date_main_str)\n",
    "\n",
    "    for window_name, duration_days, offset_days in time_windows_config:\n",
    "        start_date = reference_end_date_main - timedelta(days=offset_days + duration_days)\n",
    "        end_date = reference_end_date_main - timedelta(days=offset_days)\n",
    "        window_data = sat_df_filtered_base[\n",
    "            (sat_df_filtered_base[date_col_sat] >= start_date) & \n",
    "            (sat_df_filtered_base[date_col_sat] < end_date)\n",
    "        ].copy()\n",
    "\n",
    "        if window_data.empty:\n",
    "            # Create a DF with NaNs for all PIDs for this window's features\n",
    "            agg_data_for_window = pd.DataFrame(index=pd.Index(all_pids_from_base_str, name='PID'))\n",
    "            for fc_base in feature_cols_sat_to_process: # Use actual processable features\n",
    "                agg_data_for_window[f\"{filename_prefix}_{fc_base}_knn_mean_k{k_neighbors}_{window_name}\"] = np.nan\n",
    "        else:\n",
    "            agg_data_for_window = window_data.groupby('PID')[feature_cols_sat_to_process].apply(\n",
    "                lambda x: x.tail(k_neighbors).mean(skipna=True) \n",
    "            )\n",
    "            agg_data_for_window.columns = [\n",
    "                f\"{filename_prefix}_{c_base}_knn_mean_k{k_neighbors}_{window_name}\" \n",
    "                for c_base in agg_data_for_window.columns\n",
    "            ]\n",
    "            agg_data_for_window = agg_data_for_window.reindex(all_pids_from_base_str) # Ensure all PIDs\n",
    "        \n",
    "        all_window_features_dfs.append(agg_data_for_window.reset_index())\n",
    "\n",
    "    final_sat_df = pd.DataFrame({'PID': all_pids_from_base_str}) # Start with all PIDs as strings\n",
    "    for df_w in all_window_features_dfs:\n",
    "        if not df_w.empty and 'PID' in df_w.columns:\n",
    "            # Ensure df_w['PID'] is string if not already, for robust merge\n",
    "            df_w['PID'] = df_w['PID'].astype(str)\n",
    "            cols_to_merge = [c for c in df_w.columns if c != 'PID']\n",
    "            if cols_to_merge:\n",
    "                 final_sat_df = pd.merge(final_sat_df, df_w[['PID'] + cols_to_merge], on='PID', how='left')\n",
    "    \n",
    "    del sat_df_full, sat_df_filtered_base, window_data; gc.collect()\n",
    "    return final_sat_df\n",
    "\n",
    "\n",
    "# --- 1. Load Core Data ---\n",
    "print(\"Loading core data...\")\n",
    "train_df_orig = pd.read_csv(f'{BASE_PATH}Train.csv')\n",
    "test_df_orig = pd.read_csv(f'{BASE_PATH}Test.csv')\n",
    "\n",
    "# Ensure PIDs are strings from the beginning\n",
    "train_df_orig['PID'] = train_df_orig['PID'].astype(str)\n",
    "test_df_orig['PID'] = test_df_orig['PID'].astype(str)\n",
    "\n",
    "\n",
    "if 'BD' in train_df_orig.columns: train_df_orig.rename(columns={'BD': 'BulkDensity'}, inplace=True)\n",
    "if 'BD' in test_df_orig.columns: test_df_orig.rename(columns={'BD': 'BulkDensity'}, inplace=True)\n",
    "\n",
    "# --- 2. Combine Train and Test & Initial Preprocessing ---\n",
    "train_len = len(train_df_orig); train_df_orig['is_train'] = 1; test_df_orig['is_train'] = 0\n",
    "for nut in TARGET_NUTRIENTS:\n",
    "    if nut not in train_df_orig.columns: train_df_orig[nut] = np.nan\n",
    "    if nut not in test_df_orig.columns: test_df_orig[nut] = np.nan\n",
    "\n",
    "# original_test_bd_df PIDs will be strings\n",
    "original_test_bd_df = test_df_orig[['PID', 'BulkDensity']].copy()\n",
    "original_test_bd_df.rename(columns={'BulkDensity': 'BulkDensity_original'}, inplace=True)\n",
    "if original_test_bd_df['BulkDensity_original'].isnull().any():\n",
    "    bd_train_median_orig = train_df_orig['BulkDensity'].median(skipna=True)\n",
    "    original_test_bd_df['BulkDensity_original'].fillna(bd_train_median_orig if not pd.isna(bd_train_median_orig) else 1.2, inplace=True)\n",
    "\n",
    "combined_df = pd.concat([train_df_orig, test_df_orig], axis=0, ignore_index=True)\n",
    "# all_pids_in_dataset will be strings\n",
    "all_pids_in_dataset = combined_df['PID'].unique().tolist()\n",
    "\n",
    "\n",
    "del train_df_orig, test_df_orig; gc.collect()\n",
    "\n",
    "# --- 2.5 Add Missing Indicator Flags & Outlier Capping ---\n",
    "print(\"\\n--- Adding Missing Indicator Flags & Capping Outliers ---\")\n",
    "core_features_to_flag_missing = ['ecec20', 'hp20', 'xhp20', 'BulkDensity']\n",
    "for col in core_features_to_flag_missing:\n",
    "    if col in combined_df.columns and combined_df[col].isnull().any(): combined_df[f'{col}_is_missing'] = combined_df[col].isnull().astype(int)\n",
    "    else: combined_df[f'{col}_is_missing'] = 0\n",
    "features_for_capping_present = [f for f in FEATURES_FOR_OUTLIER_CAPPING if f in combined_df.columns]\n",
    "for col in features_for_capping_present:\n",
    "    if combined_df[col].isnull().all(): continue\n",
    "    lower_cap = combined_df.loc[combined_df['is_train'] == 1, col].quantile(LOWER_PERCENTILE_CAP)\n",
    "    upper_cap = combined_df.loc[combined_df['is_train'] == 1, col].quantile(UPPER_PERCENTILE_CAP)\n",
    "    if pd.isna(lower_cap) or pd.isna(upper_cap) or lower_cap >= upper_cap: continue\n",
    "    combined_df[col] = np.clip(combined_df[col], lower_cap, upper_cap)\n",
    "\n",
    "\n",
    "# --- 3. Satellite Data Integration ---\n",
    "print(f\"\\n--- Starting Satellite Data Integration ---\")\n",
    "satellite_files_config = [\n",
    "    (f'{BASE_PATH}LANDSAT8_data_updated.csv', 'date'), (f'{BASE_PATH}MODIS_MOD16A2_data.csv', 'date'),\n",
    "    (f'{BASE_PATH}MODIS_MCD43A4_data.csv', 'date'), (f'{BASE_PATH}MODIS_MOD09GA_data.csv', 'date'),\n",
    "    (f'{BASE_PATH}Sentinel2_data.csv', 'date'), (f'{BASE_PATH}Sentinel1_data.csv', 'date'),\n",
    "    (f'{BASE_PATH}MODIS_MOD13Q1_data.csv', 'date'), (f'{BASE_PATH}MODIS_MOD11A1_data.csv', 'date')\n",
    "]\n",
    "for sat_file_path, _ in satellite_files_config:\n",
    "    print(f\"Processing satellite file: {sat_file_path.split('/')[-1]}\")\n",
    "    # all_pids_in_dataset are already strings\n",
    "    processed_sat_df = load_and_preprocess_satellite_data_multi_window_knn_features(\n",
    "        filepath=sat_file_path, all_pids_from_base=all_pids_in_dataset,\n",
    "        reference_end_date_main_str=REFERENCE_END_DATE_OF_MAIN_WINDOW_STR,\n",
    "        time_windows_config=SATELLITE_TIME_WINDOWS, k_neighbors=K_FOR_KNN_FEATURES\n",
    "    )\n",
    "    if 'PID' in processed_sat_df.columns and not processed_sat_df.drop(columns=['PID'], errors='ignore').empty:\n",
    "        # Ensure PID in processed_sat_df is also string for merging with combined_df['PID'] (which is string)\n",
    "        processed_sat_df['PID'] = processed_sat_df['PID'].astype(str)\n",
    "        cols_before_merge = set(combined_df.columns)\n",
    "        combined_df = pd.merge(combined_df, processed_sat_df, on='PID', how='left') # Both PIDs are string\n",
    "        cols_after_merge = set(combined_df.columns)\n",
    "        newly_added_agg_cols = list(cols_after_merge - cols_before_merge)\n",
    "        for agg_col in newly_added_agg_cols:\n",
    "            if agg_col in combined_df.columns and combined_df[agg_col].isnull().any():\n",
    "                combined_df[f'{agg_col}_is_missing'] = combined_df[agg_col].isnull().astype(int)\n",
    "            else: combined_df[f'{agg_col}_is_missing'] = 0\n",
    "    del processed_sat_df; gc.collect()\n",
    "print(\"\\nFinished satellite data integration.\")\n",
    "\n",
    "\n",
    "# --- 4. Feature Engineering (Polynomials, Interactions) ---\n",
    "print(\"\\n--- Advanced Static and EO Interaction Feature Engineering ---\")\n",
    "key_static_features_for_fe = ['pH', 'soc20', 'cec20', 'BulkDensity', 'alb', 'slope']\n",
    "for col in key_static_features_for_fe:\n",
    "    if col in combined_df.columns and combined_df[col].isnull().any():\n",
    "        median_train = combined_df.loc[combined_df['is_train'] == 1, col].median()\n",
    "        combined_df[col].fillna(median_train if not pd.isna(median_train) else 0, inplace=True)\n",
    "poly_individual_candidates = ['pH', 'soc20', 'cec20', 'BulkDensity']\n",
    "poly_individual_to_use = [f for f in poly_individual_candidates if f in combined_df.columns]\n",
    "if poly_individual_to_use:\n",
    "    poly_creator_individual = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n",
    "    try:\n",
    "        source_df_for_poly_individual = combined_df[poly_individual_to_use].copy()\n",
    "        for col_p in poly_individual_to_use:\n",
    "            if source_df_for_poly_individual[col_p].isnull().any():\n",
    "                 median_val = source_df_for_poly_individual[col_p].median()\n",
    "                 source_df_for_poly_individual[col_p].fillna(median_val if not pd.isna(median_val) else 0, inplace=True)\n",
    "        poly_arr_individual = poly_creator_individual.fit_transform(source_df_for_poly_individual)\n",
    "        poly_feature_names_individual = poly_creator_individual.get_feature_names_out(input_features=poly_individual_to_use)\n",
    "        poly_df_individual = pd.DataFrame(poly_arr_individual, columns=poly_feature_names_individual, index=combined_df.index)\n",
    "        new_terms_from_poly_individual = [name for name in poly_feature_names_individual if name not in poly_individual_to_use and name not in combined_df.columns]\n",
    "        if new_terms_from_poly_individual: combined_df = pd.concat([combined_df, poly_df_individual[new_terms_from_poly_individual]], axis=1)\n",
    "    except Exception as e_poly_ind: print(f\"  Error creating individual polynomial features: {e_poly_ind}\")\n",
    "\n",
    "static_interaction_candidates = ['pH', 'soc20', 'cec20', 'BulkDensity', 'alb', 'slope']\n",
    "for col1, col2 in itertools.combinations(static_interaction_candidates, 2):\n",
    "    if col1 in combined_df.columns and col2 in combined_df.columns:\n",
    "        for c_int in [col1, col2]:\n",
    "            if combined_df[c_int].isnull().any():\n",
    "                median_val_int_train = combined_df.loc[combined_df['is_train'] == 1, c_int].median()\n",
    "                combined_df[c_int].fillna(median_val_int_train if not pd.isna(median_val_int_train) else 0, inplace=True)\n",
    "        interaction_col_name = f'{col1}_x_{col2}'\n",
    "        if interaction_col_name not in combined_df.columns: combined_df[interaction_col_name] = combined_df[col1] * combined_df[col2]\n",
    "\n",
    "example_eo_ndvi_feature = f'LANDSAT8_NDVI_calc_knn_mean_k{K_FOR_KNN_FEATURES}_win90d_mid'\n",
    "static_feat_for_eo_interaction = 'pH'\n",
    "if static_feat_for_eo_interaction in combined_df.columns and example_eo_ndvi_feature in combined_df.columns:\n",
    "    if combined_df[static_feat_for_eo_interaction].isnull().any():\n",
    "        median_static = combined_df.loc[combined_df['is_train']==1, static_feat_for_eo_interaction].median(skipna=True)\n",
    "        combined_df[static_feat_for_eo_interaction].fillna(median_static if not pd.isna(median_static) else 0, inplace=True)\n",
    "    if combined_df[example_eo_ndvi_feature].isnull().any():\n",
    "        median_eo = combined_df.loc[combined_df['is_train']==1, example_eo_ndvi_feature].median(skipna=True)\n",
    "        combined_df[example_eo_ndvi_feature].fillna(median_eo if not pd.isna(median_eo) else 0, inplace=True)\n",
    "    interaction_name_static_eo = f'{static_feat_for_eo_interaction}_x_LS8NDVI90d'\n",
    "    if interaction_name_static_eo not in combined_df.columns:\n",
    "        combined_df[interaction_name_static_eo] = combined_df[static_feat_for_eo_interaction] * combined_df[example_eo_ndvi_feature]\n",
    "\n",
    "\n",
    "# --- 5. Final Preprocessing (Label Encoding, Final Imputation) ---\n",
    "print(\"\\n--- Final Preprocessing ---\")\n",
    "# PID is already string, so it won't be in categorical_cols if we remove it based on name\n",
    "categorical_cols = combined_df.select_dtypes(include='object').columns.tolist()\n",
    "if 'PID' in categorical_cols: categorical_cols.remove('PID') # PID is string, but we don't label encode it\n",
    "if 'site' in combined_df.columns and 'site' in categorical_cols: categorical_cols.remove('site')\n",
    "for col in categorical_cols:\n",
    "    combined_df[col] = combined_df[col].fillna('Missing_Cat').astype(str)\n",
    "    le = LabelEncoder(); combined_df[col] = le.fit_transform(combined_df[col])\n",
    "\n",
    "# PID is string, so it's not numeric and won't be in features_for_model\n",
    "features_for_model = [f for f in combined_df.columns if f not in TARGET_NUTRIENTS + ['PID', 'is_train', 'site'] and pd.api.types.is_numeric_dtype(combined_df[f])]\n",
    "print(\"  Performing final median imputation...\")\n",
    "for col in features_for_model:\n",
    "    if combined_df[col].isnull().any():\n",
    "        median_val_to_use = combined_df.loc[combined_df['is_train'] == 1, col].median()\n",
    "        if pd.isna(median_val_to_use): median_val_to_use = combined_df[col].median()\n",
    "        if pd.isna(median_val_to_use): median_val_to_use = 0\n",
    "        combined_df[col].fillna(median_val_to_use, inplace=True)\n",
    "cols_to_drop_final_check = []\n",
    "for f in features_for_model:\n",
    "    if combined_df[f].isnull().all(): cols_to_drop_final_check.append(f)\n",
    "    elif combined_df.loc[combined_df['is_train']==1, f].nunique(dropna=False) <= 1 and combined_df.loc[combined_df['is_train']==1, f].notnull().any():\n",
    "        cols_to_drop_final_check.append(f)\n",
    "if cols_to_drop_final_check:\n",
    "    print(f\"  Dropping {len(cols_to_drop_final_check)} features (all NaN or no variance in train): {cols_to_drop_final_check}\")\n",
    "    features_for_model = [f for f in features_for_model if f not in cols_to_drop_final_check]\n",
    "    combined_df.drop(columns=cols_to_drop_final_check, inplace=True)\n",
    "print(f\"Number of features for modeling: {len(features_for_model)}\")\n",
    "\n",
    "\n",
    "# --- 6. Prepare Data for Models (Train/Test Split, Scaling for GLM) ---\n",
    "train_processed_unscaled = combined_df.loc[combined_df['is_train'] == 1, features_for_model + TARGET_NUTRIENTS].copy()\n",
    "# IMPORTANT: Keep PID (which is string) in test_processed_unscaled\n",
    "test_processed_unscaled = combined_df.loc[combined_df['is_train'] == 0, features_for_model + ['PID']].copy()\n",
    "\n",
    "# Data for LGBM (uses unscaled features)\n",
    "X_train_lgbm_full = train_processed_unscaled[features_for_model]\n",
    "y_train_lgbm_full = train_processed_unscaled[TARGET_NUTRIENTS]\n",
    "X_test_lgbm = test_processed_unscaled[features_for_model] # This is X_test_orig_df for pseudo-labeling\n",
    "\n",
    "# Data for GLM (needs scaling)\n",
    "print(\"  Scaling features for GLM using StandardScaler...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_glm_scaled_vals = scaler.fit_transform(X_train_lgbm_full)\n",
    "X_test_glm_scaled_vals = scaler.transform(X_test_lgbm)\n",
    "\n",
    "X_train_glm_scaled = pd.DataFrame(X_train_glm_scaled_vals, columns=features_for_model, index=X_train_lgbm_full.index)\n",
    "X_test_glm_scaled = pd.DataFrame(X_test_glm_scaled_vals, columns=features_for_model, index=X_test_lgbm.index)\n",
    "\n",
    "del combined_df; gc.collect()\n",
    "\n",
    "\n",
    "# --- 7. Model Training ---\n",
    "# PIDs are sourced from test_processed_unscaled['PID'] which are strings\n",
    "all_test_pids_final = test_processed_unscaled['PID'].astype(str).values # Already string, but enforce\n",
    "\n",
    "test_predictions_lgbm_ppm = pd.DataFrame({'PID': all_test_pids_final})\n",
    "test_predictions_glm_ppm = pd.DataFrame({'PID': all_test_pids_final})\n",
    "\n",
    "# 7.A LightGBM Models\n",
    "print(\"\\n--- Training LightGBM Models ---\")\n",
    "lgbm_cpu_params = {\n",
    "    'objective': 'rmse', 'metric': 'rmse', 'n_estimators': 1000, 'learning_rate': 0.02,\n",
    "    'feature_fraction': 0.7, 'bagging_fraction': 0.7, 'bagging_freq': 1,\n",
    "    'lambda_l1': 0.1, 'lambda_l2': 0.1, 'num_leaves': 41, 'max_depth': -1,\n",
    "    'min_child_samples': 20, 'verbose': -1, 'seed': SEED, 'boosting_type': 'gbdt', 'n_jobs': -1\n",
    "}\n",
    "for nutrient in TARGET_NUTRIENTS:\n",
    "    print(f\"  Training LGBM for Nutrient: {nutrient}\")\n",
    "    y_train_nutrient = y_train_lgbm_full[nutrient].copy()\n",
    "    valid_target_idx = y_train_nutrient.dropna().index\n",
    "\n",
    "    if len(valid_target_idx) == 0:\n",
    "        print(f\"    Skipping LGBM for {nutrient} due to no valid training targets.\")\n",
    "        test_predictions_lgbm_ppm[nutrient] = 0\n",
    "        continue\n",
    "\n",
    "    y_train_final_nutrient = y_train_nutrient.loc[valid_target_idx]\n",
    "    X_train_final_nutrient_lgbm = X_train_lgbm_full.loc[valid_target_idx]\n",
    "    selected_features_lgbm = features_for_model\n",
    "\n",
    "    if X_train_final_nutrient_lgbm[selected_features_lgbm].empty:\n",
    "        print(f\"    Skipping LGBM for {nutrient} due to empty X_train after filtering.\")\n",
    "        test_predictions_lgbm_ppm[nutrient] = 0\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        if ENABLE_PSEUDO_LABELING_LGBM:\n",
    "            print(f\"    Applying confident pseudo-labeling for LGBM on {nutrient}...\")\n",
    "            # X_test_lgbm's index is important for pseudo-labeling to map back predictions.\n",
    "            # Its index comes from test_processed_unscaled.\n",
    "            model_lgbm = confident_pseudo_labeling_lgbm( # Using the simpler PL function\n",
    "                initial_model_params=lgbm_cpu_params.copy(),\n",
    "                X_train_orig_df=X_train_final_nutrient_lgbm.copy(),\n",
    "                y_train_orig_series=y_train_final_nutrient.copy(),\n",
    "                X_test_orig_df=X_test_lgbm.copy(), # Pass features of test set\n",
    "                selected_features_list=selected_features_lgbm,\n",
    "                iterations=PSEUDO_LABELING_ITERATIONS,\n",
    "                confidence_percentile=PSEUDO_LABELING_CONFIDENCE_PERCENTILE,\n",
    "                n_bootstrap_for_confidence=PSEUDO_LABELING_N_BOOTSTRAP_CONF,\n",
    "                base_seed=SEED,\n",
    "                min_confident_samples_to_add=PSEUDO_LABELING_MIN_SAMPLES\n",
    "            )\n",
    "        else:\n",
    "            model_lgbm = lgb.LGBMRegressor(**lgbm_cpu_params)\n",
    "            model_lgbm.fit(X_train_final_nutrient_lgbm[selected_features_lgbm], y_train_final_nutrient)\n",
    "\n",
    "        preds_lgbm = model_lgbm.predict(X_test_lgbm[selected_features_lgbm])\n",
    "        test_predictions_lgbm_ppm[nutrient] = np.maximum(0, preds_lgbm)\n",
    "    except Exception as e:\n",
    "        print(f\"    Error during LGBM training for {nutrient} (pseudo-labeling: {ENABLE_PSEUDO_LABELING_LGBM}): {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        test_predictions_lgbm_ppm[nutrient] = 0\n",
    "\n",
    "# 7.B GLM (TweedieRegressor) Models\n",
    "print(\"\\n--- Training GLM (TweedieRegressor) Models ---\")\n",
    "glm_alpha = 0.1; glm_power = 2\n",
    "for nutrient in TARGET_NUTRIENTS:\n",
    "    print(f\"  Training GLM for Nutrient: {nutrient}\")\n",
    "    y_train_nutrient_glm = y_train_lgbm_full[nutrient].copy()\n",
    "    y_train_nutrient_glm = np.maximum(y_train_nutrient_glm, 1e-6 if glm_power > 0 else -np.inf)\n",
    "    valid_target_idx_glm = y_train_nutrient_glm.dropna().index\n",
    "\n",
    "    if len(valid_target_idx_glm) == 0:\n",
    "        print(f\"    Skipping GLM for {nutrient} due to no valid training targets.\")\n",
    "        test_predictions_glm_ppm[nutrient] = 0\n",
    "        continue\n",
    "\n",
    "    y_train_final_glm = y_train_nutrient_glm.loc[valid_target_idx_glm]\n",
    "    X_train_final_glm_scaled = X_train_glm_scaled.loc[valid_target_idx_glm]\n",
    "\n",
    "    if X_train_final_glm_scaled.empty:\n",
    "        print(f\"    Skipping GLM for {nutrient} due to empty X_train after filtering.\")\n",
    "        test_predictions_glm_ppm[nutrient] = 0\n",
    "        continue\n",
    "\n",
    "    model_glm = TweedieRegressor(power=glm_power, alpha=glm_alpha, link='log', max_iter=1000, tol=1e-4)\n",
    "    try:\n",
    "        model_glm.fit(X_train_final_glm_scaled[features_for_model], y_train_final_glm)\n",
    "        preds_glm = model_glm.predict(X_test_glm_scaled[features_for_model])\n",
    "        test_predictions_glm_ppm[nutrient] = np.maximum(0, preds_glm)\n",
    "    except Exception as e:\n",
    "        print(f\"    Error GLM {nutrient}: {e}\")\n",
    "        test_predictions_glm_ppm[nutrient] = 0\n",
    "\n",
    "# --- 8. Ensemble Predictions & Generate Submission ---\n",
    "print(\"\\n--- Ensembling Predictions and Generating Submission ---\")\n",
    "\n",
    "# PIDs in test_predictions_..._ppm are already string.\n",
    "# PIDs in original_test_bd_df are already string (set in Sec 1).\n",
    "original_test_bd_df['PID'] = original_test_bd_df['PID'].astype(str) # Re-ensure for safety\n",
    "\n",
    "test_predictions_ensembled_ppm = test_predictions_lgbm_ppm[['PID']].copy()\n",
    "\n",
    "for nutrient in TARGET_NUTRIENTS:\n",
    "    lgbm_preds_series = test_predictions_lgbm_ppm.get(nutrient)\n",
    "    glm_preds_series = test_predictions_glm_ppm.get(nutrient)\n",
    "\n",
    "    lgbm_preds = lgbm_preds_series.values if lgbm_preds_series is not None else np.zeros(len(test_predictions_ensembled_ppm))\n",
    "    if lgbm_preds_series is None: print(f\"Warning: LGBM preds missing for {nutrient}\")\n",
    "        \n",
    "    glm_preds = glm_preds_series.values if glm_preds_series is not None else np.zeros(len(test_predictions_ensembled_ppm))\n",
    "    if glm_preds_series is None: print(f\"Warning: GLM preds missing for {nutrient}\")\n",
    "        \n",
    "    test_predictions_ensembled_ppm[nutrient] = (WEIGHT_LGBM * lgbm_preds) + (WEIGHT_GLM * glm_preds)\n",
    "\n",
    "test_pred_ppm_long = test_predictions_ensembled_ppm.melt(\n",
    "    id_vars=['PID'], value_vars=TARGET_NUTRIENTS, var_name='Nutrient', value_name='Predicted_PPM')\n",
    "# test_pred_ppm_long['PID'] is string\n",
    "\n",
    "print(f\"Debug section 8 - PIDs dtypes before merge 1:\\n\"\n",
    "      f\"  test_pred_ppm_long['PID']: {test_pred_ppm_long['PID'].dtype}\\n\"\n",
    "      f\"  original_test_bd_df['PID']: {original_test_bd_df['PID'].dtype}\")\n",
    "\n",
    "final_submission_prep_df = pd.merge(test_pred_ppm_long, original_test_bd_df, on='PID', how='left')\n",
    "bulk_density_col_for_gap = 'BulkDensity_original'\n",
    "\n",
    "try:\n",
    "    gap_test_df_orig = pd.read_csv(f'{BASE_PATH}Gap_Test.csv')\n",
    "    gap_test_df_orig['PID'] = gap_test_df_orig['PID'].astype(str)\n",
    "    gap_test_df_orig['Nutrient'] = gap_test_df_orig['Nutrient'].astype(str)\n",
    "    \n",
    "    final_submission_prep_df['Nutrient'] = final_submission_prep_df['Nutrient'].astype(str)\n",
    "    \n",
    "    print(f\"Debug section 8 - PIDs/Nutrient dtypes before merge 2:\\n\"\n",
    "          f\"  final_submission_prep_df['PID']: {final_submission_prep_df['PID'].dtype}\\n\"\n",
    "          f\"  final_submission_prep_df['Nutrient']: {final_submission_prep_df['Nutrient'].dtype}\\n\"\n",
    "          f\"  gap_test_df_orig['PID']: {gap_test_df_orig['PID'].dtype}\\n\"\n",
    "          f\"  gap_test_df_orig['Nutrient']: {gap_test_df_orig['Nutrient'].dtype}\")\n",
    "\n",
    "    final_submission_prep_df = pd.merge(final_submission_prep_df, gap_test_df_orig[['PID', 'Nutrient', 'Required']], on=['PID', 'Nutrient'], how='left')\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: Gap_Test.csv not found. 'Required' column will be set to 0.\")\n",
    "    final_submission_prep_df['Required'] = 0\n",
    "except Exception as e_gap:\n",
    "    print(f\"Error loading or merging Gap_Test.csv: {e_gap}. 'Required' column will be set to 0.\")\n",
    "    if 'Required' not in final_submission_prep_df.columns: final_submission_prep_df['Required'] = 0\n",
    "\n",
    "if bulk_density_col_for_gap not in final_submission_prep_df.columns:\n",
    "    final_submission_prep_df[bulk_density_col_for_gap] = 1.2\n",
    "final_submission_prep_df[bulk_density_col_for_gap].fillna(1.2, inplace=True)\n",
    "\n",
    "final_submission_prep_df['Predicted_PPM'].fillna(0, inplace=True)\n",
    "\n",
    "if 'Required' not in final_submission_prep_df.columns:\n",
    "    final_submission_prep_df['Required'] = 0\n",
    "final_submission_prep_df['Required'].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "final_submission_prep_df['Available_kg_ha'] = final_submission_prep_df['Predicted_PPM'] * SOIL_DEPTH_CM * final_submission_prep_df[bulk_density_col_for_gap] * 0.1\n",
    "final_submission_prep_df['Gap'] = final_submission_prep_df['Required'] - final_submission_prep_df['Available_kg_ha']\n",
    "final_submission_prep_df['Gap'].fillna(0, inplace=True)\n",
    "\n",
    "final_submission_prep_df['ID'] = final_submission_prep_df['PID'].astype(str) + '_' + final_submission_prep_df['Nutrient'].astype(str)\n",
    "submission_df_final = final_submission_prep_df[['ID', 'Gap']].copy() # Use .copy()\n",
    "\n",
    "# Final check for missing IDs\n",
    "num_expected_rows = len(test_processed_unscaled['PID'].unique()) * len(TARGET_NUTRIENTS)\n",
    "if len(submission_df_final) != num_expected_rows:\n",
    "    print(f\"WARNING: Row count mismatch! Expected {num_expected_rows}, Got {len(submission_df_final)}\")\n",
    "    # Create a DataFrame of all expected IDs\n",
    "    expected_pids_final = test_processed_unscaled['PID'].astype(str).unique()\n",
    "    expected_ids_df = pd.DataFrame(list(itertools.product(expected_pids_final, TARGET_NUTRIENTS)), columns=['PID', 'Nutrient'])\n",
    "    expected_ids_df['ID_expected'] = expected_ids_df['PID'] + '_' + expected_ids_df['Nutrient']\n",
    "    \n",
    "    missing_in_submission = set(expected_ids_df['ID_expected']) - set(submission_df_final['ID'])\n",
    "    if missing_in_submission:\n",
    "        print(f\"IDs expected but missing in final submission (first 10): {list(missing_in_submission)[:10]}\")\n",
    "        if any('ID_ZgrspC' in mid for mid in missing_in_submission):\n",
    "             print(\"One of the problematic 'ID_ZgrspC' IDs is missing.\")\n",
    "\n",
    "\n",
    "submission_filename = f'submission_full_v_simpler_pl_robust_pid.csv'\n",
    "submission_df_final.to_csv(submission_filename, index=False)\n",
    "print(f\"\\nSubmission file '{submission_filename}' created successfully.\")\n",
    "print(f\"Number of rows in submission: {len(submission_df_final)}\")\n",
    "print(submission_df_final.head())\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T23:03:06.965909Z",
     "iopub.status.busy": "2025-06-22T23:03:06.965355Z",
     "iopub.status.idle": "2025-06-22T23:03:07.273333Z",
     "shell.execute_reply": "2025-06-22T23:03:07.272447Z",
     "shell.execute_reply.started": "2025-06-22T23:03:06.965869Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb # For XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T23:03:18.763931Z",
     "iopub.status.busy": "2025-06-22T23:03:18.763627Z",
     "iopub.status.idle": "2025-06-22T23:15:50.721030Z",
     "shell.execute_reply": "2025-06-22T23:15:50.720037Z",
     "shell.execute_reply.started": "2025-06-22T23:03:18.763908Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training XGBoost Models ---\n",
      "  Training XGBoost for Nutrient: N\n",
      "  Training XGBoost for Nutrient: P\n",
      "  Training XGBoost for Nutrient: K\n",
      "  Training XGBoost for Nutrient: Ca\n",
      "  Training XGBoost for Nutrient: Mg\n",
      "  Training XGBoost for Nutrient: S\n",
      "  Training XGBoost for Nutrient: Zn\n",
      "  Training XGBoost for Nutrient: Mn\n",
      "  Training XGBoost for Nutrient: Fe\n",
      "  Training XGBoost for Nutrient: Cu\n",
      "  Training XGBoost for Nutrient: B\n"
     ]
    }
   ],
   "source": [
    "# --- 7.C XGBoost Models ---\n",
    "print(\"\\n--- Training XGBoost Models ---\")\n",
    "test_predictions_xgb_ppm = pd.DataFrame({'PID': test_processed_unscaled['PID'].values})\n",
    "\n",
    "# Define XGBoost parameters\n",
    "# Note: 'early_stopping_rounds' is removed from here.\n",
    "# If you want to use it, you must provide an 'eval_set' to the fit method.\n",
    "xgb_constructor_params = {\n",
    "    'objective': 'reg:squarederror',  # For regression\n",
    "    'eval_metric': 'rmse',            # Evaluation metric\n",
    "    'eta': 0.02,                      # Learning rate\n",
    "    'max_depth': 7,                   # Maximum depth of a tree\n",
    "    'subsample': 0.8,                 # Subsample ratio of the training instance\n",
    "    'colsample_bytree': 0.8,          # Subsample ratio of columns when constructing each tree\n",
    "    'lambda': 1,                      # L2 regularization term on weights (xgb's lambda)\n",
    "    'alpha': 0.1,                     # L1 regularization term on weights (xgb's alpha)\n",
    "    'seed': SEED,\n",
    "    'n_estimators': 1000,             # Number of boosting rounds\n",
    "    'n_jobs': -1                      # Use all available CPU cores\n",
    "}\n",
    "# If you want to use early stopping, define it separately\n",
    "# XGB_EARLY_STOPPING_ROUNDS = 50\n",
    "\n",
    "for nutrient in TARGET_NUTRIENTS:\n",
    "    print(f\"  Training XGBoost for Nutrient: {nutrient}\")\n",
    "    y_train_nutrient_xgb = y_train_lgbm_full[nutrient].copy()\n",
    "    valid_target_idx_xgb = y_train_nutrient_xgb.dropna().index\n",
    "\n",
    "    if len(valid_target_idx_xgb) == 0:\n",
    "        print(f\"    Skipping XGBoost for {nutrient} due to no valid training targets.\")\n",
    "        test_predictions_xgb_ppm[nutrient] = 0\n",
    "        continue\n",
    "\n",
    "    y_train_final_nutrient_xgb = y_train_nutrient_xgb.loc[valid_target_idx_xgb]\n",
    "    X_train_final_nutrient_xgb = X_train_lgbm_full.loc[valid_target_idx_xgb, features_for_model]\n",
    "\n",
    "    if X_train_final_nutrient_xgb.empty:\n",
    "        print(f\"    Skipping XGBoost for {nutrient} due to empty X_train after filtering.\")\n",
    "        test_predictions_xgb_ppm[nutrient] = 0\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "\n",
    "        model_xgb = xgb.XGBRegressor(**xgb_constructor_params)\n",
    "\n",
    "\n",
    "        model_xgb.fit(X_train_final_nutrient_xgb, y_train_final_nutrient_xgb, verbose=False)\n",
    "\n",
    "\n",
    "        preds_xgb = model_xgb.predict(X_test_lgbm[features_for_model])\n",
    "        test_predictions_xgb_ppm[nutrient] = np.maximum(0, preds_xgb)\n",
    "    except Exception as e:\n",
    "        print(f\"    Error during XGBoost training for {nutrient}: {e}\")\n",
    "        test_predictions_xgb_ppm[nutrient] = 0\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T00:12:33.023022Z",
     "iopub.status.busy": "2025-06-23T00:12:33.022610Z",
     "iopub.status.idle": "2025-06-23T00:12:33.027869Z",
     "shell.execute_reply": "2025-06-23T00:12:33.027030Z",
     "shell.execute_reply.started": "2025-06-23T00:12:33.022977Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Ensemble Weights\n",
    "WEIGHT_LGBM = 0.25  \n",
    "WEIGHT_GLM = 0.25   \n",
    "WEIGHT_XGB = 0.5   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T00:12:40.895434Z",
     "iopub.status.busy": "2025-06-23T00:12:40.895101Z",
     "iopub.status.idle": "2025-06-23T00:12:41.055243Z",
     "shell.execute_reply": "2025-06-23T00:12:41.053984Z",
     "shell.execute_reply.started": "2025-06-23T00:12:40.895410Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Ensembling Predictions and Generating Submission (LGBM + GLM + XGB) ---\n",
      "Debug section 8 - PIDs dtypes before merge 1:\n",
      "  test_pred_ppm_long['PID']: object\n",
      "  original_test_bd_df['PID']: object\n",
      "Debug section 8 - PIDs/Nutrient dtypes before merge 2:\n",
      "  final_submission_prep_df['PID']: object\n",
      "  final_submission_prep_df['Nutrient']: object\n",
      "  gap_test_df_orig['PID']: object\n",
      "  gap_test_df_orig['Nutrient']: object\n",
      "\n",
      "Submission file 'submission_lfffffdddff_glm0.1_psodo222xgb0.4_ensemble_psl_final.csv' created successfully.\n",
      "Number of rows in submission: 26598\n",
      "            ID          Gap\n",
      "0  ID_NGS9Bx_N -3418.173747\n",
      "1  ID_YdVKXw_N -3544.720503\n",
      "2  ID_MZAlfE_N -3553.700064\n",
      "3  ID_GwCCMN_N -3426.018703\n",
      "4  ID_K8sowf_N -4059.313219\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 8. Ensemble Predictions & Generate Submission ---\n",
    "print(\"\\n--- Ensembling Predictions and Generating Submission (LGBM + GLM + XGB) ---\")\n",
    "\n",
    "# PIDs in test_predictions_..._ppm are already string (from Section 7 init).\n",
    "# PIDs in original_test_bd_df are already string (set in Sec 1).\n",
    "original_test_bd_df['PID'] = original_test_bd_df['PID'].astype(str) # Re-ensure for safety\n",
    "\n",
    "# Create ensembled DataFrame starting with PIDs from one of the prediction DFs\n",
    "# All prediction DFs (LGBM, GLM, XGB) should have the same PIDs in the same order\n",
    "test_predictions_ensembled_ppm = test_predictions_lgbm_ppm[['PID']].copy() # Has string PIDs\n",
    "\n",
    "for nutrient in TARGET_NUTRIENTS:\n",
    "    lgbm_preds_series = test_predictions_lgbm_ppm.get(nutrient)\n",
    "    glm_preds_series = test_predictions_glm_ppm.get(nutrient)\n",
    "    xgb_preds_series = test_predictions_xgb_ppm.get(nutrient) # Get XGBoost predictions\n",
    "\n",
    "    # Use .values to ensure alignment if Series come from different sources\n",
    "    # Default to an array of zeros if a prediction series is somehow missing\n",
    "    # (shouldn't happen if columns were initialized with 0 for failed nutrients)\n",
    "    len_preds = len(test_predictions_ensembled_ppm) # Number of test samples\n",
    "\n",
    "    lgbm_preds = lgbm_preds_series.values if lgbm_preds_series is not None else np.zeros(len_preds)\n",
    "    if lgbm_preds_series is None: print(f\"Warning: LGBM preds series missing for {nutrient}\")\n",
    "        \n",
    "    glm_preds = glm_preds_series.values if glm_preds_series is not None else np.zeros(len_preds)\n",
    "    if glm_preds_series is None: print(f\"Warning: GLM preds series missing for {nutrient}\")\n",
    "\n",
    "    xgb_preds = xgb_preds_series.values if xgb_preds_series is not None else np.zeros(len_preds)\n",
    "    if xgb_preds_series is None: print(f\"Warning: XGB preds series missing for {nutrient}\")\n",
    "        \n",
    "    # Ensemble with XGBoost\n",
    "    test_predictions_ensembled_ppm[nutrient] = (\n",
    "        WEIGHT_LGBM * lgbm_preds +\n",
    "        WEIGHT_GLM * glm_preds +\n",
    "        WEIGHT_XGB * xgb_preds\n",
    "    )\n",
    "\n",
    "test_pred_ppm_long = test_predictions_ensembled_ppm.melt(\n",
    "    id_vars=['PID'], value_vars=TARGET_NUTRIENTS, var_name='Nutrient', value_name='Predicted_PPM')\n",
    "# test_pred_ppm_long['PID'] is string\n",
    "\n",
    "print(f\"Debug section 8 - PIDs dtypes before merge 1:\\n\"\n",
    "      f\"  test_pred_ppm_long['PID']: {test_pred_ppm_long['PID'].dtype}\\n\"\n",
    "      f\"  original_test_bd_df['PID']: {original_test_bd_df['PID'].dtype}\")\n",
    "\n",
    "final_submission_prep_df = pd.merge(test_pred_ppm_long, original_test_bd_df, on='PID', how='left')\n",
    "bulk_density_col_for_gap = 'BulkDensity_original' \n",
    "\n",
    "try:\n",
    "    gap_test_df_orig = pd.read_csv(f'{BASE_PATH}Gap_Test.csv')\n",
    "    gap_test_df_orig['PID'] = gap_test_df_orig['PID'].astype(str) # Ensure string PID\n",
    "    gap_test_df_orig['Nutrient'] = gap_test_df_orig['Nutrient'].astype(str) # Ensure string Nutrient\n",
    "    \n",
    "    # Ensure Nutrient in final_submission_prep_df is also string for the merge\n",
    "    final_submission_prep_df['Nutrient'] = final_submission_prep_df['Nutrient'].astype(str)\n",
    "    \n",
    "    print(f\"Debug section 8 - PIDs/Nutrient dtypes before merge 2:\\n\"\n",
    "          f\"  final_submission_prep_df['PID']: {final_submission_prep_df['PID'].dtype}\\n\"\n",
    "          f\"  final_submission_prep_df['Nutrient']: {final_submission_prep_df['Nutrient'].dtype}\\n\"\n",
    "          f\"  gap_test_df_orig['PID']: {gap_test_df_orig['PID'].dtype}\\n\"\n",
    "          f\"  gap_test_df_orig['Nutrient']: {gap_test_df_orig['Nutrient'].dtype}\")\n",
    "\n",
    "    final_submission_prep_df = pd.merge(final_submission_prep_df, gap_test_df_orig[['PID', 'Nutrient', 'Required']], on=['PID', 'Nutrient'], how='left')\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: Gap_Test.csv not found. 'Required' column will be set to 0.\")\n",
    "    final_submission_prep_df['Required'] = 0 # Create column if not exists\n",
    "except Exception as e_gap:\n",
    "    print(f\"Error loading or merging Gap_Test.csv: {e_gap}. 'Required' column will be set to 0.\")\n",
    "    if 'Required' not in final_submission_prep_df.columns: final_submission_prep_df['Required'] = 0\n",
    "\n",
    "# NaN Filling (as per your working code's logic, simplified)\n",
    "if bulk_density_col_for_gap not in final_submission_prep_df.columns: # Should exist due to merge\n",
    "    final_submission_prep_df[bulk_density_col_for_gap] = 1.2\n",
    "final_submission_prep_df[bulk_density_col_for_gap].fillna(1.2, inplace=True)\n",
    "\n",
    "# Predicted_PPM can be NaN if a model failed for a nutrient. Fill with 0.\n",
    "final_submission_prep_df['Predicted_PPM'].fillna(0, inplace=True)\n",
    "\n",
    "# Required can be NaN if a PID-Nutrient pair from test set isn't in Gap_Test.csv. Fill with 0.\n",
    "if 'Required' not in final_submission_prep_df.columns: # If Gap_Test.csv failed to load/merge\n",
    "    final_submission_prep_df['Required'] = 0\n",
    "final_submission_prep_df['Required'].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "final_submission_prep_df['Available_kg_ha'] = final_submission_prep_df['Predicted_PPM'] * SOIL_DEPTH_CM * final_submission_prep_df[bulk_density_col_for_gap] * 0.1\n",
    "final_submission_prep_df['Gap'] = final_submission_prep_df['Required'] - final_submission_prep_df['Available_kg_ha']\n",
    "# Ensure Gap itself is not NaN\n",
    "final_submission_prep_df['Gap'].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "final_submission_prep_df['ID'] = final_submission_prep_df['PID'].astype(str) + '_' + final_submission_prep_df['Nutrient'].astype(str)\n",
    "submission_df_final = final_submission_prep_df[['ID', 'Gap']].copy() # Use .copy()\n",
    "\n",
    "# Final check for missing IDs\n",
    "# Assuming test_processed_unscaled is available from Section 6\n",
    "num_expected_rows = len(test_processed_unscaled['PID'].unique()) * len(TARGET_NUTRIENTS)\n",
    "if len(submission_df_final) != num_expected_rows:\n",
    "    print(f\"WARNING: Row count mismatch! Expected {num_expected_rows}, Got {len(submission_df_final)}\")\n",
    "    # Create a DataFrame of all expected IDs\n",
    "    expected_pids_final = test_processed_unscaled['PID'].astype(str).unique()\n",
    "    expected_ids_df = pd.DataFrame(list(itertools.product(expected_pids_final, TARGET_NUTRIENTS)), columns=['PID', 'Nutrient'])\n",
    "    expected_ids_df['ID_expected'] = expected_ids_df['PID'] + '_' + expected_ids_df['Nutrient']\n",
    "    \n",
    "    missing_in_submission = set(expected_ids_df['ID_expected']) - set(submission_df_final['ID'])\n",
    "    if missing_in_submission:\n",
    "        print(f\"IDs expected but missing in final submission (first 10): {list(missing_in_submission)[:10]}\")\n",
    "        # Example check for a specific problematic ID if you know one\n",
    "        # if any('ID_ZgrspC' in mid for mid in missing_in_submission):\n",
    "        #      print(\"One of the problematic 'ID_ZgrspC' IDs is missing.\")\n",
    "\n",
    "submission_filename = f'submission_lfffffdddff_glm0.1_psodo222xgb0.4_ensemble_psl_final.csv' # Updated filename\n",
    "submission_df_final.to_csv(submission_filename, index=False)\n",
    "print(f\"\\nSubmission file '{submission_filename}' created successfully.\")\n",
    "print(f\"Number of rows in submission: {len(submission_df_final)}\")\n",
    "print(submission_df_final.head())\n",
    "print(\"\\nDone!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7028267,
     "isSourceIdPinned": false,
     "sourceId": 11480796,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
